version: '3.8'

services:
  db:
    container_name: database-ingestion
    image: postgres
    networks:
      - airflow_net
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: dataflow
    ports:
      - "5432:5432"
    volumes:
      - ./database/init.sql:/docker-entrypoint-initdb.d/init.sql
      - pgdata:/var/lib/postgresql/data

  minio:
    container_name: datalake
    image: minio/minio
    networks:
      - airflow_net
    environment:
      MINIO_ACCESS_KEY: minio
      MINIO_SECRET_KEY: minio123
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - miniodata:/data

  airflow:
    image: apache/airflow:2.9.1
    container_name: airflow
    restart: always
    depends_on:
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow.db
    volumes:
      - ./data/csv:/data/csv
      - airflow_data:/opt/airflow
      - ./dags:/opt/airflow/dags
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "8080:8080"
    command: webserver
    user: "0:0"

  airflow-scheduler:
    image: apache/airflow:2.9.1
    container_name: airflow-scheduler
    restart: always
    depends_on:
      - airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow.db
    volumes:
      - airflow_data:/opt/airflow
      - ./dags:/opt/airflow/dags
      - /var/run/docker.sock:/var/run/docker.sock
    command: scheduler
    user: "0:0"

  airflow-init:
    image: apache/airflow:2.9.1
    container_name: airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: sqlite:////opt/airflow/airflow.db
    volumes:
      - airflow_data:/opt/airflow
      - ./dags:/opt/airflow/dags
      - /var/run/docker.sock:/var/run/docker.sock
    entrypoint: >
      bash -c "
        airflow db init &&
        airflow users create --username admin --firstname admin --lastname admin --role Admin --email admin@example.com --password admin
      "
    user: "0:0"
  
  kafka:
    image: bitnami/kafka:latest
    container_name: kafka
    networks:
      - airflow_net
    environment:
      KAFKA_CFG_NODE_ID: 1
      KAFKA_CFG_PROCESS_ROLES: broker,controller
      KAFKA_CFG_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
      KAFKA_CFG_LISTENERS: PLAINTEXT://:9092,CONTROLLER://:9093
      KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT
      KAFKA_CFG_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      ALLOW_PLAINTEXT_LISTENER: "yes"
    ports:
      - "9092:9092"
    healthcheck:
      test: ["CMD", "kafka-topics.sh", "--bootstrap-server", "localhost:9092", "--list"]
      interval: 10s
      timeout: 5s
      retries: 10

  api_producer:
    container_name: api_producer
    image: dataflow_api_producer
    build:
      context: ./ingestion/producers/api
      dockerfile: Dockerfile.api_producer
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - airflow_net
    restart: on-failure

  api_consumer:
    container_name: api_consumer
    image: dataflow_api_consumer
    build:
      context: ./ingestion/consumers/api
      dockerfile: Dockerfile.api_consumer
    depends_on:
      kafka:
        condition: service_healthy
      minio:
        condition: service_started
    networks:
      - airflow_net
    restart: on-failure

  csv_producer:
    container_name: csv_producer
    image: dataflow_csv_producer
    build:
      context: ./ingestion/producers/csv
      dockerfile: Dockerfile.csv_producer
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - airflow_net
    restart: on-failure

  csv_consumer:
    container_name: csv_consumer
    image: dataflow_csv_consumer
    build:
      context: ./ingestion/consumers/csv
      dockerfile: Dockerfile.csv_consumer
    depends_on:
      kafka:
        condition: service_healthy
      minio:
        condition: service_started
    networks:
      - airflow_net
    restart: on-failure

  database_producer:
    container_name: database_producer
    image: dataflow_database_producer
    build:
      context: ./ingestion/producers/database
      dockerfile: Dockerfile.database_producer
    depends_on:
      db:
        condition: service_started
      kafka:
        condition: service_healthy
    networks:
      - airflow_net
    volumes:
      - ./ingestion/producers/database:/app
    restart: on-failure

  database_consumer:
    container_name: database_consumer
    image: dataflow_database_consumer
    build:
      context: ./ingestion/consumers/database
      dockerfile: Dockerfile.database_consumer
    depends_on:
      - minio
      - kafka
    networks:
      - airflow_net

  csv_transform:
    container_name: csv_transform
    image: dataflow_csv_transform
    build:
      context: ./transformers/csv
      dockerfile: Dockerfile.transform_csv
    depends_on:
      db:
        condition: service_started
      minio:
        condition: service_started
      kafka:
        condition: service_healthy
    volumes:
      - ./transformers/csv:/app
    restart: on-failure
    networks:
      - airflow_net

  database_transform:
    container_name: database_transform
    image: dataflow_database_transform
    build:
      context: ./transformers/database
      dockerfile: Dockerfile.database_transform
    depends_on:
      db:
        condition: service_started
      minio:
        condition: service_started
      kafka:
        condition: service_healthy
    volumes:
      - ./transformers/database:/app
    restart: on-failure

  api_transform:
    container_name: api_transform
    image: dataflow_api_transform
    build:
      context: ./transformers/api
      dockerfile: Dockerfile.transform_api
    depends_on:
      db:
        condition: service_started
      minio:
        condition: service_started
      kafka:
        condition: service_healthy
    volumes:
      - ./transformers/parquet:/app
    restart: on-failure
    networks:
      - airflow_net

networks:
  airflow_net:

volumes:
  airflow_data:
  pgdata:
  miniodata:
  dags: